{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q learning with different exploration strategies\n",
    "\n",
    "Authors: [johnjim0816](https://github.com/johnjim0816)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、定义算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "class QLearning(object):\n",
    "    def __init__(self,cfg):\n",
    "        self.explore_type = cfg.explore_type # 探索策略类型\n",
    "        self.n_actions = cfg.n_actions \n",
    "        self.lr = cfg.lr  # 学习率\n",
    "        self.gamma = cfg.gamma  \n",
    "        self.epsilon = cfg.epsilon_start\n",
    "        self.sample_count = 0  \n",
    "        self.epsilon_start = cfg.epsilon_start\n",
    "        self.epsilon_end = cfg.epsilon_end\n",
    "        self.epsilon_decay = cfg.epsilon_decay\n",
    "        ## TODO: 这里Q表可以换成数组比如np.zeros((n_states, n_actions))\n",
    "        self.Q_table  = defaultdict(lambda: np.zeros(self.n_actions)) # 用嵌套字典存放状态->动作->状态-动作值（Q值）的映射，即Q表\n",
    "    def sample_action(self, state):\n",
    "        ''' 采样动作，训练时用\n",
    "        '''\n",
    "        self.sample_count += 1\n",
    "        if self.explore_type == 'epsilon_greedy':\n",
    "            self.epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \\\n",
    "                math.exp(-1. * self.sample_count / self.epsilon_decay) # epsilon是会递减的，这里选择指数递减\n",
    "            # e-greedy 策略\n",
    "            if np.random.uniform(0, 1) > self.epsilon:\n",
    "                action = np.argmax(self.Q_table[str(state)]) # 选择Q(s,a)最大对应的动作\n",
    "            else:\n",
    "                action = np.random.choice(self.n_actions) # 随机选择动作\n",
    "            return action\n",
    "        elif self.explore_type == 'boltzmann':\n",
    "            # boltzmann 策略\n",
    "            action_probs = np.exp(self.Q_table[str(state)] / self.epsilon) / np.sum(np.exp(self.Q_table[str(state)] / self.epsilon))\n",
    "            action = np.random.choice(self.n_actions, p=action_probs)\n",
    "            return action\n",
    "        elif self.explore_type == 'ucb':\n",
    "            # ucb 策略\n",
    "            if self.sample_count < self.n_actions:\n",
    "                action = self.sample_count\n",
    "            else:\n",
    "                action = np.argmax(self.Q_table[str(state)] + self.epsilon * np.sqrt(np.log(self.sample_count) / self.sample_count))\n",
    "            return action\n",
    "        elif self.explore_type == 'softmax':\n",
    "            # softmax 策略\n",
    "            action_probs = np.exp(self.Q_table[str(state)] / self.epsilon) / np.sum(np.exp(self.Q_table[str(state)] / self.epsilon))\n",
    "            action = np.random.choice(self.n_actions, p=action_probs)\n",
    "            return action\n",
    "        elif self.explore_type == 'thompson':\n",
    "            # thompson 策略\n",
    "            action = np.argmax(np.random.beta(self.Q_table[str(state)] + 1, 1))\n",
    "            return action\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    def predict_action(self,state):\n",
    "        ''' 预测或选择动作，测试时用\n",
    "        '''\n",
    "        if self.explore_type == 'epsilon_greedy':\n",
    "            action = np.argmax(self.Q_table[str(state)])\n",
    "            return action\n",
    "        elif self.explore_type == 'boltzmann':\n",
    "            action_probs = np.exp(self.Q_table[str(state)] / self.epsilon) / np.sum(np.exp(self.Q_table[str(state)] / self.epsilon))\n",
    "            action = np.random.choice(self.n_actions, p=action_probs)\n",
    "            return action\n",
    "        elif self.explore_type == 'ucb':\n",
    "            action = np.argmax(self.Q_table[str(state)])\n",
    "            return action\n",
    "        elif self.explore_type == 'softmax':\n",
    "            action_probs = np.exp(self.Q_table[str(state)] / self.epsilon) / np.sum(np.exp(self.Q_table[str(state)] / self.epsilon))\n",
    "            action = np.random.choice(self.n_actions, p=action_probs)\n",
    "            return action\n",
    "        elif self.explore_type == 'thompson':\n",
    "            action = np.argmax(np.random.beta(self.Q_table[str(state)] + 1, 1))\n",
    "            return action\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    def update(self, state, action, reward, next_state, terminated):\n",
    "        Q_predict = self.Q_table[str(state)][action] \n",
    "        if terminated: # 终止状态\n",
    "            Q_target = reward  \n",
    "        else:\n",
    "            Q_target = reward + self.gamma * np.max(self.Q_table[str(next_state)]) \n",
    "        self.Q_table[str(state)][action] += self.lr * (Q_target - Q_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、定义训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg,env,agent):\n",
    "    print('开始训练！')\n",
    "    print(f'环境:{cfg.env_name}, 算法:{cfg.algo_name}, 设备:{cfg.device}')\n",
    "    rewards = []  # 记录奖励\n",
    "    for i_ep in range(cfg.train_eps):\n",
    "        ep_reward = 0  # 记录每个回合的奖励\n",
    "        state = env.reset(seed=cfg.seed)  # 重置环境,即开始新的回合\n",
    "        while True:\n",
    "            action = agent.sample_action(state)  # 根据算法采样一个动作\n",
    "            next_state, reward, terminated, info = env.step(action)  # 与环境进行一次动作交互\n",
    "            agent.update(state, action, reward, next_state, terminated)  # Q学习算法更新\n",
    "            state = next_state  # 更新状态\n",
    "            ep_reward += reward\n",
    "            if terminated:\n",
    "                break\n",
    "        rewards.append(ep_reward)\n",
    "        if (i_ep+1)%20==0:\n",
    "            print(f\"回合：{i_ep+1}/{cfg.train_eps}，奖励：{ep_reward:.1f}，Epsilon：{agent.epsilon:.3f}\")\n",
    "    print('完成训练！')\n",
    "    return {\"rewards\":rewards} #TODO:可以加收敛的回合数\n",
    "def test(cfg,env,agent):\n",
    "    print('开始测试！')\n",
    "    print(f'环境：{cfg.env_name}, 算法：{cfg.algo_name}, 设备：{cfg.device}')\n",
    "    rewards = []  # 记录所有回合的奖励\n",
    "    for i_ep in range(cfg.test_eps):\n",
    "        ep_reward = 0  # 记录每个episode的reward\n",
    "        state = env.reset(seed=cfg.seed)  # 重置环境, 重新开一局（即开始新的一个回合）\n",
    "        while True:\n",
    "            action = agent.predict_action(state)  # 根据算法选择一个动作\n",
    "            next_state, reward, terminated, info = env.step(action)  # 与环境进行一个交互\n",
    "            state = next_state  # 更新状态\n",
    "            ep_reward += reward\n",
    "            if terminated:\n",
    "                break\n",
    "        rewards.append(ep_reward)\n",
    "        print(f\"回合数：{i_ep+1}/{cfg.test_eps}, 奖励：{ep_reward:.1f}\")\n",
    "    print('完成测试！')\n",
    "    return {\"rewards\":rewards}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、定义环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'discrete' from 'gym.envs.toy_text' (/Users/jj/opt/anaconda3/envs/easyrl/lib/python3.7/site-packages/gym/envs/toy_text/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ft/40bs0pj97sb5m_1pwrzxjgwh0000gn/T/ipykernel_82755/2410913532.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoy_text\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFrozenLakeEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimple_grid\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDrunkenWalkEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mlake_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrozenLakeEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_slippery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0malley_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDrunkenWalkEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'theAlley'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/rl-tutorials/notebooks/envs/simple_grid.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoy_text\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdiscrete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'0.21.0'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mLEFT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'discrete' from 'gym.envs.toy_text' (/Users/jj/opt/anaconda3/envs/easyrl/lib/python3.7/site-packages/gym/envs/toy_text/__init__.py)"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys,os\n",
    "curr_path = os.path.abspath('')\n",
    "parent_path = os.path.dirname(curr_path)\n",
    "sys.path.append(parent_path)\n",
    "from gym.envs.toy_text import FrozenLakeEnv\n",
    "from envs.simple_grid import DrunkenWalkEnv\n",
    "lake_env = FrozenLakeEnv(is_slippery=False)\n",
    "alley_env = DrunkenWalkEnv(map_name='theAlley')\n",
    "walk_in_the_park_env = DrunkenWalkEnv(map_name='walkInThePark')\n",
    "\n",
    "env_dict = {\n",
    "    'theAlley': alley_env ,\n",
    "    'walkInThePark': walk_in_the_park_env,\n",
    "    'FrozenLakeEasy-v0': lake_env,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、设置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "class Config:\n",
    "    '''配置参数\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.env_name = 'theAlley' # 环境名称\n",
    "        self.algo_name = 'Q-Learning' # 算法名称\n",
    "        self.explore_type = 'epsilon_greedy' # 探索策略\n",
    "        self.train_eps = 400 # 训练回合数\n",
    "        self.test_eps = 20 # 测试回合数\n",
    "        self.max_steps = 200 # 每个回合最大步数\n",
    "        self.epsilon_start = 0.95 #  e-greedy策略中epsilon的初始值\n",
    "        self.epsilon_end = 0.01 #  e-greedy策略中epsilon的最终值\n",
    "        self.epsilon_decay = 300 #  e-greedy策略中epsilon的衰减率\n",
    "        self.gamma = 0.9 # 折扣因子\n",
    "        self.lr = 0.1 # 学习率\n",
    "        self.seed = 1 # 随机种子\n",
    "        self.device = torch.device('cpu')\n",
    "\n",
    "def smooth(data, weight=0.9):  \n",
    "    '''用于平滑曲线\n",
    "    '''\n",
    "    last = data[0]  # First value in the plot (first timestep)\n",
    "    smoothed = list()\n",
    "    for point in data:\n",
    "        smoothed_val = last * weight + (1 - weight) * point  # 计算平滑值\n",
    "        smoothed.append(smoothed_val)                    \n",
    "        last = smoothed_val                                \n",
    "    return smoothed\n",
    "\n",
    "def plot_rewards(rewards,title=\"learning curve\"):\n",
    "    sns.set()\n",
    "    plt.figure()  # 创建一个图形实例，方便同时多画几个图\n",
    "    plt.title(f\"{title}\")\n",
    "    plt.xlim(0, len(rewards), 10)  # 设置x轴的范围\n",
    "    plt.xlabel('epsiodes')\n",
    "    plt.plot(rewards, label='rewards')\n",
    "    plt.plot(smooth(rewards), label='smoothed')\n",
    "    plt.legend()\n",
    "def plot_table(table,title=\"Q table\"):\n",
    "    # TODO 把你的Qtableplot变成一个函数，不要写成类，输入输出自己想办法吧\n",
    "    sns.set()\n",
    "    plt.figure()  # 创建一个图形实例，方便同时多画几个图\n",
    "    plt.title(f\"{title}\")\n",
    "    plt.xlabel('state')\n",
    "    plt.ylabel('action')\n",
    "    plt.imshow(table, cmap='gray')\n",
    "    plt.colorbar()\n",
    "\n",
    "final_res = [] # 用于记录每个环境的最终结果\n",
    "cfg = Config() \n",
    "cs = Console() #TODO 不明白用这个干啥，直接print就行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5、探索策略研究"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1、softmax\n",
    "之后的探索策略都一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cfg.explore_type = 'softmax'\n",
    "\n",
    "for env_name, env in env_dict.items():\n",
    "    print('--'*45)\n",
    "    print(f'EnvName = {env_name}')\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    print(f'状态数：{n_states}, 动作数：{n_actions}')\n",
    "    setattr(cfg, 'env_name', env_name)\n",
    "    setattr(cfg, 'n_states', n_states)\n",
    "    setattr(cfg, 'n_actions', n_actions)\n",
    "    agent = QLearning(cfg)\n",
    "    final_play_res = train(env,agent) #TODO：这里输出结果用字典表示，方便后面提取数据\n",
    "    final_res_print_str = f'Method: {final_play_res[0]}, MeanStepCnt: {final_play_res[1]:.3f}, MeanReward: {final_play_res[2]:.3f}]'\n",
    "    final_res.append([env_name] + final_play_res)\n",
    "    cs.print(final_res_print_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6、总结\n",
    "这里写一下文字说明"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(final_res, columns=['envName', 'policy', 'stepCount', 'Rewards']).sort_values(by='envName')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('easyrl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8994a120d39b6e6a2ecc94b4007f5314b68aa69fc88a7f00edf21be39b41f49c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
